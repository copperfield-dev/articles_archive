# Kylin消费Kafka数据时间选取说明

通过脚本streaming_build.sh启动
> streaming_build.sh有三个参数：cube_name，interval，delay
 
## streaming_build.sh的参数解释
 
-  **-start**：*starttime = current_time - current_time % interval - delay*
-  **-end**：*endtime =  current_time - current_time % interval - delay + interval*
 
    > 所以这段segement的id为“starttime”_"endtime"
 
-  **-cube**：cubename

## 如何抓起kafka数据
 
- 有一个时间范围timeRange，也就是由命令行计算而成的参数(starttime, endtime)
- margin理解为等待时间，或是容错时间：在Load Datasource的时候修改，默认值300000(5分钟)
- 变量startTimestamp，启示的时间戳， = timeRange.getFirst() - margin
    > 比如，要算14:00~14:05的数据，那么startTimestamp=14:00-5min=15:55
 
offset需要计算获得，计算过程如下：

> - 通过Kafka的API获得earliestOffset，OffsetRequest.EarliestTime()
> - 通过Kafka的API获得latestOffset，OffsetRequest.LatestTime()
> - 利用二分查找在[earliestOffset, latestOffset]中，离startTimestamp最近的offset，查找时key值是timestamp而不是offset
 
从offset开始循环抓取，若获得的消息

> - 在timeRange范围内，加入结果队列
> - 小于timeRange.getSecond() + margin了话, 不做任何处理
> - 大于timeRange.getSecond() + margin了话，停止抓取，结束循环

## 一些问题

crontab每隔五分钟启动一次build实现自动化，可以并行处理
每次build运行成功，需要手动enable该cube，该时间段的结果才可以用
# Spark应用多种提交方案汇总

## 依赖解决

> If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster. To do this, create an assembly jar (or “uber” jar) containing your code and its dependencies.
> Both sbt and Maven have assembly plugins. When creating assembly jars, list Spark and Hadoop as provided dependencies; these need not be bundled since they are provided by the cluster manager at runtime.

总结：

1. Spark应用打包需要带上依赖的包，通过assembly的方式
2. 集群会提供Spark和Hadoop的相关依赖，打包的时候可以把Spark和Hadoop设为provided

## spark-submit

bin/spark-submit脚本提交命令:

```Shell
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
```

其他一些选项:

- --supervise: to make sure that the driver is automatically restarted if it fails with a non-zero exit code.
- --executor-memory 20G
- --total-executor-cores 100
- --num-executors 50

## 针对Local[K]模式的提交

单机启动一个进程, 通过多个线程来模拟Spark分布式计算

| Master URL | Meaning |                                                                                     
|------------|---------|
| local      | Run Spark locally with one worker thread (i.e. no parallelism at all).                              |
| local[K]   | Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). |
| local[*]   | Run Spark locally with as many worker threads as logical cores on your machine.                     |

> 运行该模式, 不用启动Spark的Master、Worker守护进程

在程序执行过程中, 只会生成一个SparkSubmit进程，这个SparkSubmit进程:
- 既是客户提交任务的Client进程
- 又是Spark的driver程序
- 还充当着Spark执行Task的Executor角色

## 针对Spark Standalone Mode的提交

### Spark Standalone Deploy

> In addition to running on the Mesos or YARN cluster managers, Spark also provides a simple standalone deploy mode. You can launch a standalone cluster either manually, by starting a master and workers by hand, or use our provided launch scripts. It is also possible to run these daemons on a single machine for testing.

需要启动Spark的master和worker进程:

```Shell
./sbin/start-all.sh

or

./sbin/start-master.sh
./sbin/start-worker.sh <master-spark-URL>
```

### spark-submit的`--master`参数

| Master URL                      | Meaning                                                                                                                                                                                                                                                                 |
|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| spark://HOST:PORT               | Connect to the given Spark standalone cluster master. The port must be whichever one your master is configured to use, which is 7077 by default.                                                                                                                          |
| spark://HOST1:PORT1,HOST2:PORT2 | Connect to the given Spark standalone cluster with standby masters with Zookeeper. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default. |

### spark-submit的`--deploy-mode`参数

#### client模式

> In client mode, the driver is launched in the same process as the client that submits the application. 

- SparkSubmit做为client端和运行Driver程序
- Worker进程启动executor来执行应用程序

> Worker进程生成几个Executor, 每个Executor使用几个core, 这些都可以在`spark-env.sh`里面配置

#### cluster模式

> In cluster mode, however, the driver is launched from one of the Worker processes inside the cluster, and the client process exits as soon as it fulfills its responsibility of submitting the application without waiting for the application to finish.
 
- SparkSubmit进程会在应用程序提交给集群之后就退出, 不会等待应用执行完成
- Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动Driver程序

> 该DriverWrapper进程会占用Worker进程的一个core，所以同样的资源下配置下，会比client少用1个core来参与计算

- 应用程序的结果, 会在执行Driver程序的节点的stdout中输出, 而不是打印在屏幕上

## 针对YARN的提交

### spark-submit的`--master`参数

| Master URL | Meaning                                                                                                                                                                                 |
|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| yarn       | Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable. |

### spark-submit的`--deploy-mode`参数

#### client模式

> In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.

- 向ResourceManager提交应用程序, 生成SparkSubmit进程, 该进程会执行Driver程序
- RM会在集群中的某个NodeManager上, 启动一个ExecutorLauncher进程, 来做为ApplicationMaster
- 在多个NodeManager上生成CoarseGrainedExecutorBackend进程来并发的执行应用程序
- 应用程序的结果, 会在终端输出

#### cluster模式

> In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application.  

- 在ResourceManager端提交应用程序, 会生成SparkSubmit进程, 该进程只用来做Client, 应用程序提交给集群后, 就会删除该进程
- ResourceManager在集群中的某个NodeManager上运行ApplicationMaster, 该AM同时会执行Driver程序
- 在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序
- 应用程序的结果, 会在执行driver程序的节点的stdout中输出, 而不是打印在屏幕上
